# Fine-tuning LLMs Guide with Modal.ai

## ðŸš€ Quick Start Checklist

**Everything runs on Modal's cloud - no local GPU or heavy installations needed!**

- [ ] Install Modal CLI: `pip install modal`
- [ ] Authenticate: `modal setup` (opens browser)
- [ ] Create test script: `test_setup.py` (verify everything works)
- [ ] Run test: `modal run test_setup.py`
- [ ] Create training script: `train_qwen_therapist.py`
- [ ] Set up HF token secret: `modal secret create hf-token HF_TOKEN=your_token`
- [ ] Run training: `modal run train_qwen_therapist.py`
- [ ] Model saved automatically to Modal volume (no download needed!)

**Key Points:**
- âœ… **No local GPU required** - everything runs on Modal's cloud GPUs
- âœ… **No local package installation** - all packages install automatically in Modal
- âœ… **Web UI authentication** - `modal setup` opens browser for easy sign-in
- âœ… **Models stay on Modal** - stored in persistent volumes, download only if needed
- âœ… **Start with test runs** - verify setup before full training

## 1. Understand Fine-tuning

Fine-tuning an LLM customizes its behavior, enhances + injects knowledge, and optimizes performance for domains/specific tasks. For example:

* **GPT-4** serves as a base model; however, OpenAI fine-tuned it to better comprehend instructions and prompts, leading to the creation of ChatGPT-4 which everyone uses today.
* **DeepSeek-R1-Distill-Llama-8B** is a fine-tuned version of Llama-3.1-8B. DeepSeek utilized data generated by DeepSeek-R1, to fine-tune Llama-3.1-8B. This process, known as distillation (a subcategory of fine-tuning), injects the data into the Llama model to learn reasoning capabilities.

With [Unsloth](https://github.com/unslothai/unsloth) and Modal.ai's powerful GPU infrastructure, you can fine-tune models efficiently on cloud GPUs without managing infrastructure. By fine-tuning a pre-trained model (e.g. Llama-3.1-8B) on a specialized dataset, you can:

* **Update + Learn New Knowledge**: Inject and learn new domain-specific information.
* **Customize Behavior**: Adjust the model's tone, personality, or response style.
* **Optimize for Tasks**: Improve accuracy and relevance for specific use cases.

**Example usecases**:

* Train LLM to predict if a headline impacts a company positively or negatively.
* Use historical customer interactions for more accurate and custom responses.
* Fine-tune LLM on legal texts for contract analysis, case law research, and compliance.

You can think of a fine-tuned model as a specialized agent designed to do specific tasks more effectively and efficiently. **Fine-tuning can replicate all of RAG's capabilities**, but not vice versa.

#### Fine-tuning misconceptions:

You may have heard that fine-tuning does not make a model learn new knowledge or RAG performs better than fine-tuning. That is **false**. Fine-tuning allows models to learn new knowledge and patterns directly, while RAG retrieves information but doesn't modify the model's weights. Fine-tuning is particularly powerful for domain-specific knowledge injection and behavior customization.

## 2. Choose the Right Model + Method

If you're a beginner, it is best to start with a small instruct model like Llama 3.1 (8B) and experiment from there. You'll also need to decide between QLoRA and LoRA training:

* **LoRA:** Fine-tunes small, trainable matrices in 16-bit without updating all model weights.
* **QLoRA:** Combines LoRA with 4-bit quantization to handle very large models with minimal resources.

You can change the model name to whichever model you like by matching it with model's name on Hugging Face e.g. `'unsloth/llama-3.1-8b-unsloth-bnb-4bit'`.

We recommend starting with **Instruct models**, as they allow direct fine-tuning using conversational chat templates (ChatML, ShareGPT etc.) and require less data compared to **Base models** (which uses Alpaca, Vicuna etc).

* Model names ending in **`unsloth-bnb-4bit`** indicate they are [**Unsloth dynamic 4-bit**](https://unsloth.ai/blog/dynamic-4bit) **quants**. These models consume slightly more VRAM than standard BitsAndBytes 4-bit models but offer significantly higher accuracy.
* If a model name ends with just **`bnb-4bit`**, without "unsloth", it refers to a standard BitsAndBytes 4-bit quantization.
* Models with **no suffix** are in their original **16-bit or 8-bit formats**. While they are the original models from the official model creators, we sometimes include important fixes - such as chat template or tokenizer fixes. So it's recommended to use our versions when available.

There are other settings which you can toggle:

* **`max_seq_length = 2048`** â€“ Controls context length. While Llama-3 supports 8192, we recommend 2048 for testing. Unsloth enables 4Ã— longer context fine-tuning.
* **`dtype = None`** â€“ Defaults to None; use `torch.float16` or `torch.bfloat16` for newer GPUs.
* **`load_in_4bit = True`** â€“ Enables 4-bit quantization, reducing memory use 4Ã— for fine-tuning. Disabling it enables LoRA 16-bit fine-tuning. You can also enable 16-bit LoRA with `load_in_16bit = True`
* To enable full fine-tuning (FFT), set `full_finetuning = True`. For 8-bit fine-tuning, set `load_in_8bit = True`.
* **Note:** Only one training method can be set to `True` at a time.

We recommend starting with QLoRA, as it is one of the most accessible and effective methods for training models. Our [dynamic 4-bit](https://unsloth.ai/blog/dynamic-4bit) quants, the accuracy loss for QLoRA compared to LoRA is now largely recovered.

You can also do [Text-to-speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), [reasoning (GRPO)](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), [vision](https://docs.unsloth.ai/basics/vision-fine-tuning), [reinforcement learning](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/reinforcement-learning-dpo-orpo-and-kto) (DPO, ORPO, KTO), [continued pretraining](https://docs.unsloth.ai/basics/continued-pretraining), text completion and other training methodologies with Unsloth.

### Popular Model Options for Fine-tuning

**Small Models (1.5B - 7B parameters)** - Great for:
- Fast training and inference
- Lower GPU memory requirements
- Domain-specific tasks
- Cost-effective experimentation

* **Qwen Models:**
  - `Qwen/Qwen2.5-1.5B-Instruct` - Ultra-small, great for quick experiments
  - `Qwen/Qwen2.5-3B-Instruct` - Balanced performance and efficiency
  - `Qwen/Qwen2.5-7B-Instruct` - Strong performance, still manageable
  - `Qwen/Qwen2.5-14B-Instruct` - Higher capability, requires more resources

* **Llama Models:**
  - `unsloth/Llama-3.1-8B-Instruct-bnb-4bit` - Popular choice, well-supported
  - `unsloth/Llama-3.2-3B-Instruct-bnb-4bit` - Smaller alternative

* **Other Options:**
  - `mistralai/Mistral-7B-Instruct-v0.3` - Strong instruction following
  - `microsoft/Phi-3-mini-4k-instruct` - Very efficient, Microsoft's compact model

**Medium Models (13B - 30B parameters)** - For:
- More complex reasoning tasks
- Better instruction following
- Higher quality outputs

**Large Models (70B+ parameters)** - For:
- State-of-the-art performance
- Complex multi-step reasoning
- Research and production applications

### Common Use Cases

1. **Therapeutic/Mental Health Chatbots**
   - Fine-tune on therapist-client conversation datasets
   - Generate empathetic, contextually appropriate responses
   - Support mental health applications and training tools

2. **Domain-Specific Assistants**
   - Legal document analysis and contract review
   - Medical Q&A and patient information
   - Financial analysis and market prediction
   - Technical documentation and code assistance

3. **Custom Behavior & Tone**
   - Adjust personality and response style
   - Brand voice alignment
   - Cultural and linguistic adaptation

4. **Task-Specific Optimization**
   - Code generation and debugging
   - Content creation (blogs, articles, stories)
   - Translation and multilingual support
   - Summarization and extraction

## 3. Your Dataset

For LLMs, datasets are collections of data that can be used to train our models. In order to be useful for training, text data needs to be in a format that can be tokenized.

* You will need to create a dataset usually with 2 columns - question and answer. The quality and amount will largely reflect the end result of your fine-tune so it's imperative to get this part right.
* You can [synthetically generate data](https://docs.unsloth.ai/get-started/datasets-guide#synthetic-data-generation) and structure your dataset (into QA pairs) using ChatGPT or local LLMs.
* You can also use our new Synthetic Dataset notebook which automatically parses documents (PDFs, videos etc.), generates QA pairs and auto cleans data using local models like Llama 3.2.
* Fine-tuning can learn from an existing repository of documents and continuously expand its knowledge base, but just dumping data alone won't work as well. For optimal results, curate a well-structured dataset, ideally as question-answer pairs. This enhances learning, understanding, and response accuracy.
* But, that's not always the case, e.g. if you are fine-tuning a LLM for code, just dumping all your code data can actually enable your model to yield significant performance improvements, even without structured formatting. So it really depends on your use case.

For most examples, we utilize the [Alpaca dataset](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-6.-alpaca-dataset) however other notebooks like Vision will use different datasets which may need images in the answer output as well.

**Dataset Format for Modal.ai:**
- Store your dataset as JSON, JSONL, or CSV files
- Upload to Modal.ai volumes or reference from cloud storage (S3, GCS, etc.)
- Ensure your dataset is accessible from your Modal.ai functions

**Additional Resources:**
- **[Hugging Face Dataset Creator Skill](https://github.com/huggingface/skills/tree/main/hf_dataset_creator)**: Templates and scripts for creating structured training datasets with prompts and formatting helpers.

### Example: Loading a Hugging Face Dataset

For datasets hosted on Hugging Face Hub, you can load them directly:

```python
from datasets import load_dataset

# Login first using: huggingface-cli login
# Or programmatically: from huggingface_hub import login; login()

# Load a dataset from Hugging Face Hub
ds = load_dataset("Jyz1331/therapist_conversations")

# Inspect the dataset structure
print(ds)
print(ds['train'][0])  # See first example
```

**Note:** Some datasets may require authentication. Use `huggingface-cli login` or set your `HF_TOKEN` environment variable.

## 4. Understand Training Hyperparameters

Learn how to choose the right hyperparameters using best practices from research and real-world experiments - and understand how each one affects your model's performance.

**Key Hyperparameters:**

* **`per_device_train_batch_size = 2`** â€“ Increase for better GPU utilization but beware of slower training due to padding. Instead, increase `gradient_accumulation_steps` for smoother training.
* **`gradient_accumulation_steps = 4`** â€“ Simulates a larger batch size without increasing memory usage.
* **`max_steps = 60`** â€“ Speeds up training. For full runs, replace with `num_train_epochs = 1` (1â€“3 epochs recommended to avoid overfitting).
* **`learning_rate = 2e-4`** â€“ Lower for slower but more precise fine-tuning. Try values like `1e-4`, `5e-5`, or `2e-5`.
* **`warmup_steps = 10`** â€“ Gradually increases learning rate at the start of training.
* **`logging_steps = 1`** â€“ How often to log training metrics.
* **`save_steps = 50`** â€“ How often to save checkpoints.

We generally recommend keeping the default settings unless you need longer training or larger batch sizes.

**Cost Estimation:**
Before training, estimate your costs using tools from the [Hugging Face LLM Trainer Skill](https://github.com/huggingface/skills/tree/main/hf-llm-trainer), which includes cost estimators to help you budget for GPU usage on Modal.ai.

## 5. Installing + Requirements for Modal.ai

### Step-by-Step Setup (Everything Runs on Modal - No Local Installation Needed!)

**Important:** All code runs remotely on Modal's cloud infrastructure. You don't need to install packages locally or have GPUs. Everything happens in the cloud!

#### Step 1: Install Modal CLI (Local - Just for Authentication)

You only need Modal CLI for authentication. Install it:

```bash
pip install modal
```

#### Step 2: Authenticate via Web UI

Run this command - it will open your browser for authentication:

```bash
modal setup
```

This will:
- Open your browser to Modal's authentication page
- Let you sign in with your account (Google, GitHub, etc.)
- Automatically configure authentication
- **No tokens to manage manually!**

#### Step 3: Choose Your GPU

Modal provides serverless GPU infrastructure. Choose based on your model size:

- **A10G** (24GB VRAM) - Good for 7B-13B models with QLoRA (e.g., Qwen2.5-7B, Llama-3.1-8B)
- **A100** (40GB/80GB VRAM) - Best for larger models (13B-30B) or full fine-tuning
- **H100** (80GB VRAM) - For the largest models (70B+) and fastest training

**GPU Recommendations by Model Size:**
- **1.5B-3B models** (Qwen2.5-1.5B, Qwen2.5-3B): A10G or even T4 (16GB) with QLoRA
- **7B-8B models** (Qwen2.5-7B, Llama-3.1-8B): A10G with QLoRA
- **13B-14B models**: A10G with QLoRA, or A100 for better performance
- **30B+ models**: A100 or H100 required

#### Step 4: Create Your First Test Script

Create a file `test_setup.py` to verify everything works:

```python
import modal

# Define the image with all dependencies
# Everything installs automatically in Modal's cloud - no local installation needed!
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git",
        "transformers",
        "datasets",
        "trl",
        "peft",
        "accelerate",
        "bitsandbytes",
        "xformers",
        "huggingface_hub",
    )
)

app = modal.App("test-finetune-setup")

# Create a volume for storing models and data
# This persists across runs - your trained models stay here!
training_volume = modal.Volume.from_name("training-data", create_if_missing=True)

@app.function(
    image=image,
    gpu=modal.gpu.A10G(),  # Start with A10G for testing
    timeout=600,  # 10 minutes for test
    volumes={"/data": training_volume},
)
def test_setup():
    """Test that all packages are installed correctly"""
    print("Testing Modal setup...")
    
    # Test imports
    from unsloth import FastLanguageModel
    from datasets import load_dataset
    import torch
    
    print("âœ“ All packages imported successfully!")
    print(f"âœ“ PyTorch version: {torch.__version__}")
    print(f"âœ“ CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    
    # Test volume access
    import os
    test_file = "/data/test.txt"
    with open(test_file, "w") as f:
        f.write("Volume test successful!")
    
    print(f"âœ“ Volume write test successful!")
    
    return "Setup test passed! Ready for fine-tuning."

# Entry point - runs remotely on Modal
@app.local_entrypoint()
def main():
    result = test_setup.remote()
    print(result)
```

#### Step 5: Run Your First Test

Run the test script to verify everything works:

```bash
modal run test_setup.py
```

This will:
- Build the container image in Modal's cloud (first time takes a few minutes)
- Install all packages remotely
- Run the test function on a GPU
- Show you the output

**Expected output:**
```
Testing Modal setup...
âœ“ All packages imported successfully!
âœ“ PyTorch version: 2.x.x
âœ“ CUDA available: True
âœ“ GPU: NVIDIA A10G
âœ“ Volume write test successful!
Setup test passed! Ready for fine-tuning.
```

**If you see errors:** Check that `modal setup` completed successfully and you're authenticated.

#### Step 6: Understanding Modal Volumes (Where Your Models Live)

Modal Volumes are persistent storage that survives between runs. This is where you'll store:
- Your trained models
- Training checkpoints
- Datasets (if you upload them)
- Any other persistent data

**Important:** Everything runs on Modal - models are stored on Modal volumes, not your local machine!

**Volume Commands:**
```bash
# List your volumes
modal volume list

# Download a model from volume (if needed)
modal volume get training-data /qwen_therapist_lora ./local_model

# Upload files to volume (if needed)
modal volume put training-data ./my_dataset.jsonl /datasets/
```

## 6. Training + Evaluation on Modal.ai

### Step 7: Run a Small Test Training First!

Before training on your full dataset, run a quick test with just a few examples to verify everything works:

```python
import modal
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import torch

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git",
        "transformers",
        "datasets",
        "trl",
        "peft",
        "accelerate",
        "bitsandbytes",
        "xformers",
        "huggingface_hub",
    )
)

app = modal.App("test-training")

training_volume = modal.Volume.from_name("training-data", create_if_missing=True)

@app.function(
    image=image,
    gpu=modal.gpu.A10G(),
    timeout=1800,  # 30 minutes for test
    volumes={"/data": training_volume},
    secrets=[modal.Secret.from_name("hf-token")],  # For accessing Hugging Face datasets
)
def test_training():
    """Quick test training with just 10 examples"""
    print("Starting test training...")
    
    # Load a small model for testing
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",  # Very small for quick test
        max_seq_length=512,  # Shorter for testing
        dtype=None,
        load_in_4bit=True,
    )
    
    model = FastLanguageModel.get_peft_model(
        model,
        r=8,  # Lower rank for testing
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_alpha=8,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing=True,
    )
    
    # Create tiny test dataset
    test_data = {
        "instruction": ["What is AI?"] * 5,
        "output": ["AI is artificial intelligence..."] * 5
    }
    from datasets import Dataset
    test_dataset = Dataset.from_dict(test_data)
    
    def format_func(examples):
        texts = []
        for inst, out in zip(examples["instruction"], examples["output"]):
            text = tokenizer.apply_chat_template(
                [{"role": "user", "content": inst},
                 {"role": "assistant", "content": out}],
                tokenize=False,
                add_generation_prompt=False,
            )
            texts.append(text)
        return {"text": texts}
    
    test_dataset = test_dataset.map(format_func, batched=True)
    
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=test_dataset,
        dataset_text_field="text",
        max_seq_length=512,
        args=TrainingArguments(
            per_device_train_batch_size=1,
            gradient_accumulation_steps=2,
            max_steps=5,  # Just 5 steps for testing!
            learning_rate=2e-4,
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=1,
            output_dir="/data/test_output",
        ),
    )
    
    trainer.train()
    print("âœ“ Test training completed successfully!")
    return "Test passed! Ready for full training."

@app.local_entrypoint()
def main():
    result = test_training.remote()
    print(result)
```

Run this test first:
```bash
modal run test_training.py
```

This verifies:
- âœ… Model loading works
- âœ… Training loop works
- âœ… Volume storage works
- âœ… Everything runs on Modal

**Once this works, proceed to full training!**

### Example 1: Fine-tuning Qwen for Therapist Conversations

Here's the complete example for fine-tuning Qwen2.5-7B on the therapist conversations dataset. **Everything runs remotely on Modal - no local execution needed!**

**First, set up your Hugging Face token as a Modal secret:**

1. Go to [Hugging Face Settings > Tokens](https://huggingface.co/settings/tokens)
2. Create a new token (read access is enough for public datasets)
3. In Modal web UI or via CLI, create a secret:
   ```bash
   modal secret create hf-token HF_TOKEN=your_token_here
   ```

Now create `train_qwen_therapist.py`:

```python
import modal
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import torch

# All packages install automatically in Modal's cloud
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git",
        "transformers",
        "datasets",
        "trl",
        "peft",
        "accelerate",
        "bitsandbytes",
        "xformers",
        "huggingface_hub",
    )
)

app = modal.App("qwen-therapist-finetune")

# Persistent volume - your model stays here!
training_volume = modal.Volume.from_name("training-data", create_if_missing=True)

@app.function(
    image=image,
    gpu=modal.gpu.A10G(),  # A10G is sufficient for Qwen2.5-7B with QLoRA
    timeout=7200,  # 2 hours
    secrets=[modal.Secret.from_name("hf-token")],  # For accessing Hugging Face datasets
    volumes={"/data": training_volume},
)
def train_qwen_therapist():
    """Fine-tune Qwen2.5-7B on therapist conversations - runs entirely on Modal!"""
    # Load Qwen model - using 7B for good balance of performance and efficiency
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="Qwen/Qwen2.5-7B-Instruct",
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,  # Use QLoRA for efficiency
    )
    
    # Enable optimizations
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                       "gate_proj", "up_proj", "down_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing=True,
        random_state=3407,
    )
    
    # Load therapist conversations dataset
    dataset = load_dataset("Jyz1331/therapist_conversations")
    
    # Inspect dataset structure
    print(f"Dataset keys: {dataset.keys()}")
    print(f"First example: {dataset['train'][0] if 'train' in dataset else dataset[list(dataset.keys())[0]][0]}")
    
    # Format dataset for Qwen's chat template
    # Adjust this based on your dataset's actual structure
    def formatting_prompts_func(examples):
        texts = []
        # Adapt based on your dataset structure - common formats:
        # Option 1: If dataset has 'conversation' or 'messages' field
        if "conversation" in examples:
            for conv in examples["conversation"]:
                text = tokenizer.apply_chat_template(
                    conv,
                    tokenize=False,
                    add_generation_prompt=False,
                )
                texts.append(text)
        # Option 2: If dataset has 'instruction' and 'output' fields
        elif "instruction" in examples and "output" in examples:
            for instruction, output in zip(examples["instruction"], examples["output"]):
                text = tokenizer.apply_chat_template(
                    [{"role": "user", "content": instruction},
                     {"role": "assistant", "content": output}],
                    tokenize=False,
                    add_generation_prompt=False,
                )
                texts.append(text)
        # Option 3: If dataset has 'user' and 'assistant' fields
        elif "user" in examples and "assistant" in examples:
            for user, assistant in zip(examples["user"], examples["assistant"]):
                text = tokenizer.apply_chat_template(
                    [{"role": "user", "content": user},
                     {"role": "assistant", "content": assistant}],
                    tokenize=False,
                    add_generation_prompt=False,
                )
                texts.append(text)
        else:
            # Fallback: inspect and adapt
            raise ValueError(f"Unknown dataset format. Keys: {examples.keys()}")
        
        return {"text": texts}
    
    # Get the training split
    train_dataset = dataset["train"] if "train" in dataset else dataset[list(dataset.keys())[0]]
    
    # Format the dataset
    train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
    
    # Training arguments optimized for therapeutic conversations
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        dataset_text_field="text",
        max_seq_length=2048,
        packing=False,
        args=TrainingArguments(
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            num_train_epochs=1,  # Start with 1 epoch, increase if needed
            learning_rate=2e-4,
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs",
            save_steps=100,
        ),
    )
    
    # Train
    trainer.train()
    
    # Save model
    model.save_pretrained("/data/qwen_therapist_lora")
    tokenizer.save_pretrained("/data/qwen_therapist_lora")
    
    # Optimize for inference
    FastLanguageModel.for_inference(model)
    
    return "Training complete! Model saved to /data/qwen_therapist_lora"

# Entry point - everything runs remotely on Modal
@app.local_entrypoint()
def main():
    print("Starting training on Modal...")
    print("This will run entirely in the cloud - no local GPU needed!")
    result = train_qwen_therapist.remote()
    print(result)
    print("\nâœ“ Training complete! Model saved to Modal volume: /data/qwen_therapist_lora")
    print("  Access it via: modal volume list")
    print("  Download if needed: modal volume get training-data /qwen_therapist_lora ./local_model")
```

**To run this:**
```bash
modal run train_qwen_therapist.py
```

**What happens:**
1. Modal builds the container image (first time: ~5-10 minutes)
2. All packages install automatically in the cloud
3. Training runs on Modal's GPU
4. Model saves to Modal volume (persistent storage)
5. **Everything stays on Modal - nothing downloads to your machine unless you explicitly download it**

**Key Points for Therapist Conversation Fine-tuning:**
- **Model Choice**: Qwen2.5-7B-Instruct is ideal - good balance of capability and efficiency
- **Dataset**: The `Jyz1331/therapist_conversations` dataset contains real therapist-client dialogues
- **Use Case**: Creates a specialized model for empathetic, contextually appropriate therapeutic responses
- **GPU**: A10G (24GB) is sufficient for Qwen2.5-7B with QLoRA
- **Ethical Considerations**: Always include disclaimers that this is not a replacement for professional therapy

### Complete Training Script Example (Llama)

Here's a complete example of fine-tuning with Unsloth on Modal.ai using Llama:

```python
import modal
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
import torch

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git",
        "transformers",
        "datasets",
        "trl",
        "peft",
        "accelerate",
        "bitsandbytes",
        "xformers",
    )
)

app = modal.App("fine-tuning-llm")

@app.function(
    image=image,
    gpu=modal.gpu.A10G(),  # or A100(), H100()
    timeout=7200,  # 2 hours
    volumes={"/data": modal.Volume.from_name("training-data", create_if_missing=True)},
)
def train_model():
    # Load model and tokenizer
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/llama-3.1-8b-unsloth-bnb-4bit",
        max_seq_length=2048,
        dtype=None,  # Auto detection
        load_in_4bit=True,
    )
    
    # Enable gradient checkpointing and other optimizations
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,  # LoRA rank
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                       "gate_proj", "up_proj", "down_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing=True,
        random_state=3407,
    )
    
    # Load your dataset
    from datasets import load_dataset
    dataset = load_dataset("json", data_files="/data/training_data.jsonl", split="train")
    
    # Format dataset for chat template
    def formatting_prompts_func(examples):
        inputs = examples["instruction"]
        outputs = examples["output"]
        texts = []
        for input, output in zip(inputs, outputs):
            text = tokenizer.apply_chat_template(
                [{"role": "user", "content": input},
                 {"role": "assistant", "content": output}],
                tokenize=False,
                add_generation_prompt=False,
            )
            texts.append(text)
        return {"text": texts}
    
    dataset = dataset.map(formatting_prompts_func, batched=True)
    
    # Training arguments
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=2048,
        packing=False,
        args=TrainingArguments(
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            max_steps=60,  # For testing; use num_train_epochs=1 for full training
            learning_rate=2e-4,
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs",
            save_steps=50,
        ),
    )
    
    # Train
    trainer.train()
    
    # Save model
    model.save_pretrained("/data/lora_model")
    tokenizer.save_pretrained("/data/lora_model")
    
    # For inference optimization
    FastLanguageModel.for_inference(model)
    
    return "Training complete! Model saved to /data/lora_model"

@app.local_entrypoint()
def main():
    result = train_model.remote()
    print(result)
```

### Running Your Training (All Remote on Modal!)

**Step 1: Run your training script**
```bash
modal run train_qwen_therapist.py
```

This command:
- âœ… Runs everything remotely on Modal's GPUs
- âœ… No local execution needed
- âœ… Streams logs to your terminal
- âœ… Saves model to Modal volume automatically

**Step 2: Monitor training**

While training runs, you can:
- **Watch logs in terminal** - Training loss and progress appear in real-time
- **Check Modal Dashboard** - Visit [modal.com](https://modal.com) to see:
  - GPU usage
  - Training progress
  - Logs and metrics
  - Cost tracking

**Expected training loss:**
- For many cases, a loss around 0.5 to 1.0 is a good sign
- Loss should decrease over time
- If loss goes to 0, you might be overfitting

**Step 3: Access your trained model**

Your model is stored on Modal volume at `/data/qwen_therapist_lora`. You can:

**Option A: Keep it on Modal (Recommended)**
- Model stays on Modal volume
- Use it for inference on Modal (see next section)
- No download needed!

**Option B: Download to local machine (if needed)**
```bash
modal volume get training-data /qwen_therapist_lora ./local_model
```

**Option C: Push to Hugging Face Hub**
Add this to your training function:
```python
from huggingface_hub import login
login(token=os.environ["HF_TOKEN"])  # From your secret
model.push_to_hub("your-username/qwen-therapist", tokenizer=tokenizer)
```

### Evaluation

In order to evaluate, you could do manual evaluation by chatting with the model and see if it's to your liking. You can also enable evaluation in the training script:

```python
# Add evaluation dataset
eval_dataset = load_dataset("json", data_files="/data/eval_data.jsonl", split="train")

# Update TrainingArguments
args=TrainingArguments(
    # ... existing args ...
    evaluation_strategy="steps",
    eval_steps=100,
    per_device_eval_batch_size=2,
)

# Update trainer
trainer = SFTTrainer(
    # ... existing args ...
    eval_dataset=eval_dataset,
)
```

For testing, you can also take 20% of your training data and use that for testing. You can also use automatic eval tools like EleutherAI's [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

**Additional Evaluation Resources:**
- **[HF Model Evaluation Skill](https://github.com/huggingface/skills/tree/main/hf_model_evaluation)**: Provides instructions and utilities for orchestrating evaluation jobs, generating reports, and mapping metrics. This can help automate and standardize your evaluation workflow.

## 7. Running + Saving the Model (All on Modal!)

### Inference on Modal.ai (Remote Execution)

After training, create an inference function that runs entirely on Modal. **No need to download the model!**

Create `inference.py`:

```python
@app.function(
    image=image,
    gpu=modal.gpu.A10G(),
    volumes={"/data": training_volume},  # Same volume where model is stored
    timeout=300,  # 5 minutes for inference
)
def inference(prompt: str):
    """Run inference using the fine-tuned model - all on Modal!"""
    from unsloth import FastLanguageModel
    import torch
    
    print(f"Loading model from Modal volume...")
    # Load the fine-tuned model from Modal volume
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="/data/qwen_therapist_lora",  # Path on Modal volume
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )
    
    # Enable fast inference
    FastLanguageModel.for_inference(model)
    
    # Format prompt
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")
    
    # Generate
    print("Generating response...")
    outputs = model.generate(
        input_ids=inputs,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

@app.local_entrypoint()
def main():
    # Test with a therapeutic prompt
    prompt = "I've been feeling really anxious lately. Can you help me understand what might be causing this?"
    print(f"Prompt: {prompt}\n")
    result = inference.remote(prompt)
    print(f"\nResponse: {result}")
```

**Run inference:**
```bash
modal run inference.py
```

**What happens:**
- âœ… Model loads from Modal volume (no download needed!)
- âœ… Inference runs on Modal's GPU
- âœ… Response returned to your terminal
- âœ… Everything stays on Modal

### Saving the Model (All on Modal Volumes!)

**Your models are automatically saved to Modal volumes during training.** Here are your options:

**Option 1: Keep on Modal Volume (Recommended)**
```python
# This happens automatically in your training function
model.save_pretrained("/data/qwen_therapist_lora")
tokenizer.save_pretrained("/data/qwen_therapist_lora")
```

**Benefits:**
- âœ… Persistent storage - survives between runs
- âœ… Fast access for inference on Modal
- âœ… No download needed
- âœ… Can share with other Modal functions

**Access your volume:**
```bash
# List volumes
modal volume list

# See what's in your volume
modal volume ls training-data
```

**Option 2: Push to Hugging Face Hub (for sharing/deployment)**
```python
from huggingface_hub import login
import os

# Use your HF token from Modal secret
login(token=os.environ["HF_TOKEN"])

# Push model
model.push_to_hub("your-username/qwen-therapist", tokenizer=tokenizer)
```

**Option 3: Download to Local Machine (if needed)**
```bash
# Download entire model directory
modal volume get training-data /qwen_therapist_lora ./local_model

# Download specific files
modal volume get training-data /qwen_therapist_lora/adapter_config.json ./
```

**Option 4: Export for Other Inference Engines**

Add this to your training function to export in different formats:

```python
# For Ollama (GGUF format)
model.save_pretrained_gguf("/data/gguf_model", tokenizer)

# For vLLM
model.save_pretrained("/data/vllm_model", tokenizer=tokenizer)

# All formats stay on Modal volume until you download them
```

**Note:** The [HF LLM Trainer Skill](https://github.com/huggingface/skills/tree/main/hf-llm-trainer) includes a `convert_to_gguf.py` helper script that can assist with model conversion for various inference engines.

**Best Practice:** Keep models on Modal volumes for inference, only download when you need to deploy elsewhere or share with others.

### Using the Model in Other Inference Engines

After saving, you can use your fine-tuned model in:
- **Ollama**: Convert to GGUF format
- **vLLM**: Use the saved model directory
- **Open WebUI**: Import the model
- **Hugging Face Spaces**: Push to hub and deploy

## 8. Quick Reference: Models & Use Cases

### Model Selection Guide

| Model | Size | Best For | GPU Needed | Training Time* |
|-------|------|----------|------------|----------------|
| **Qwen2.5-1.5B-Instruct** | 1.5B | Quick experiments, simple tasks | A10G/T4 | ~30 min |
| **Qwen2.5-3B-Instruct** | 3B | Balanced performance/speed | A10G | ~45 min |
| **Qwen2.5-7B-Instruct** | 7B | **Therapist conversations**, general tasks | A10G | ~1-2 hrs |
| **Llama-3.1-8B-Instruct** | 8B | General purpose, well-supported | A10G | ~1-2 hrs |
| **Qwen2.5-14B-Instruct** | 14B | Higher quality, complex reasoning | A100 | ~3-4 hrs |
| **Llama-3.1-70B-Instruct** | 70B | State-of-the-art performance | A100/H100 | ~8-12 hrs |

*Training times are approximate for ~1000 examples with QLoRA on Modal.ai

### Use Case Examples

**1. Therapeutic/Mental Health Chatbots**
- **Model**: Qwen2.5-7B-Instruct or Llama-3.1-8B-Instruct
- **Dataset**: `Jyz1331/therapist_conversations` or similar
- **Output**: Empathetic, contextually appropriate therapeutic responses
- **Example**: Fine-tune on therapist-client dialogues for mental health support applications

**2. Code Generation & Assistance**
- **Model**: Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct
- **Dataset**: Code completion pairs, GitHub code, documentation
- **Output**: Context-aware code suggestions and explanations
- **Example**: Fine-tune on Python documentation and examples

**3. Legal Document Analysis**
- **Model**: Qwen2.5-14B-Instruct or Llama-3.1-70B-Instruct
- **Dataset**: Legal contracts, case law, compliance documents
- **Output**: Contract analysis, case law research, compliance checking
- **Example**: Fine-tune on legal Q&A pairs for contract review

**4. Medical Q&A**
- **Model**: Qwen2.5-7B-Instruct or larger
- **Dataset**: Medical textbooks, patient Q&A, clinical notes
- **Output**: Medical information retrieval and patient education
- **Example**: Fine-tune on medical knowledge bases (with proper disclaimers)

**5. Customer Support**
- **Model**: Qwen2.5-3B-Instruct or Qwen2.5-7B-Instruct
- **Dataset**: Historical customer interactions, support tickets
- **Output**: Brand-aligned customer service responses
- **Example**: Fine-tune on company-specific support conversations

**6. Content Creation**
- **Model**: Qwen2.5-7B-Instruct
- **Dataset**: Blog posts, articles, creative writing examples
- **Output**: Brand-voice aligned content generation
- **Example**: Fine-tune on your company's blog posts and articles

**7. Translation & Multilingual**
- **Model**: Qwen2.5-7B-Instruct (excellent multilingual support)
- **Dataset**: Parallel translation corpora
- **Output**: Domain-specific translation (e.g., medical, legal)
- **Example**: Fine-tune on specialized translation pairs

### Dataset Format Quick Reference

Most models expect one of these formats:

**Format 1: Instruction-Output**
```json
{"instruction": "What is therapy?", "output": "Therapy is a treatment method..."}
```

**Format 2: User-Assistant (Chat)**
```json
{"user": "I'm feeling anxious", "assistant": "I understand. Let's explore..."}
```

**Format 3: Messages (Multi-turn)**
```json
{"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

**Format 4: Conversation**
```json
{"conversation": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

### Cost Estimates (Modal.ai)

Approximate costs for fine-tuning on Modal.ai (as of 2025):

- **A10G**: ~$0.75-1.00/hour
- **A100 (40GB)**: ~$2.50-3.00/hour  
- **A100 (80GB)**: ~$3.50-4.00/hour
- **H100**: ~$8.00-10.00/hour

**Example**: Fine-tuning Qwen2.5-7B on A10G for 2 hours â‰ˆ $1.50-2.00

## 9. We're done!

You've successfully fine-tuned a language model on Modal.ai and exported it to your desired inference engine with Unsloth!

### Next Steps

- Experiment with different hyperparameters
- Try larger models or different base models
- Fine-tune for specific domains or tasks
- Deploy your model as a Modal.ai endpoint for production use

### Resources

* **Unsloth Documentation**: https://docs.unsloth.ai/
* **Unsloth Blog**: https://unsloth.ai/blog/
* **Modal.ai Documentation**: https://modal.com/docs
* **Unsloth Discord**: https://discord.gg/unsloth
* **Unsloth Reddit**: https://www.reddit.com/r/unsloth
* **[Hugging Face Skills Repository](https://github.com/huggingface/skills)**: Comprehensive training resources including:
  - **[HF LLM Trainer Skill](https://github.com/huggingface/skills/tree/main/hf-llm-trainer)**: Training guidance, helper scripts (`train_sft_example.py`, `convert_to_gguf.py`), and cost estimators
  - **[HF Dataset Creator Skill](https://github.com/huggingface/skills/tree/main/hf_dataset_creator)**: Templates and scripts for creating structured training datasets
  - **[HF Model Evaluation Skill](https://github.com/huggingface/skills/tree/main/hf_model_evaluation)**: Evaluation orchestration and reporting utilities

### Tips for Modal.ai

1. **Cost Optimization**: Use spot instances when available to reduce costs
2. **Volume Management**: Regularly clean up old checkpoints to save storage costs
3. **Monitoring**: Use Modal.ai's dashboard to monitor GPU usage and costs
4. **Scaling**: Modal.ai automatically scales, but you can set concurrency limits
5. **Timeouts**: Set appropriate timeouts based on your training duration

Thanks for reading and hopefully this was helpful!

