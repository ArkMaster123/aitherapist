# AI Model Configuration
# Options: 
#   - OpenAI: openai/gpt-4o-mini, openai/gpt-4o, openai/gpt-4-turbo
#   - Anthropic: anthropic/claude-sonnet-4, anthropic/claude-opus-4
#   - Custom vLLM: Full URL like https://your-workspace--example-vllm-inference-serve.modal.run
AI_MODEL=openai/gpt-4o-mini

# Optional: Custom OpenAI-compatible API endpoint (for vLLM, etc.)
# If using vLLM on Modal, set this to your Modal endpoint URL
OPENAI_API_BASE=

# Optional: API Key (not needed for vLLM, required for OpenAI/Anthropic if not using Vercel AI Gateway)
OPENAI_API_KEY=

# Optional: Enable tool calling (only works if your vLLM server has --enable-auto-tool-choice set)
# Set to 'true' to enable tools like ASCII art generator (requires server support)
ENABLE_TOOLS=false

# Optional: Vercel KV for server-side rate limiting
# KV_REST_API_URL=
# KV_REST_API_TOKEN=

